## 海量数据去重解决方案

[toc]

### 一.业务背景

> 完整代码 [big-file-solution/src/main/java/cn/xiaomurui/demo/bigFile · 小木蕊/demo - 码云 - 开源中国 (gitee.com)](https://gitee.com/poxiao02/demo/tree/master/big-file-solution/src/main/java/cn/xiaomurui/demo/bigFile)

现在有10个服务器，每个服务器有10个大文件，每个大文件有若干行字符串，现在需要对每个大文件进行去重,并将其合并为一个大文件。每一个大文件无法一次性导入内存中。类似的还有10个大文件去重后输出出现次数最多的前世10个查询串。

### 二. 解决方案

首先需要搞清楚此类问题的难点是什么？在于无法一次导入内存进行操作，所以要依据这个点来切入。将大文件才分为小文件处理，最后将处理的结果合并。这是整体的思路

#### 2.1  大文件读取

单个大文件无法一次读入内存，我们可以限制 `BufferedReader `读入的行数控制在内存可以接收的范围，同时开始处理去重逻辑，直接使用 `HashSet`去重，将完成的部分用`BufferWriter`写入临时文件。

~~~java
// 读入大文件内容
private static List<File> processFile(String filePath) throws IOException {
        List<File> tempFiles = new ArrayList<>();
        Set<String> uniqueStrings = new HashSet<>();
        int batchSize = 100000; // 每批次处理的行数，可根据内存调整
        int count = 0;

        try (BufferedReader reader = new BufferedReader(new FileReader(filePath))) {
            String line;
            while ((line = reader.readLine()) != null) {
                uniqueStrings.add(line);
                count++;
                if (count >= batchSize) {
                    tempFiles.add(writeTempFile(uniqueStrings));
                    uniqueStrings.clear(); //  清空HashSet 
                    count = 0;
                }
            }
            if (!uniqueStrings.isEmpty()) {
                tempFiles.add(writeTempFile(uniqueStrings));
            }
        } catch (FileNotFoundException e) {
            System.out.println("File not found: " + filePath);
        }

        return tempFiles;
    }
~~~

#### 2.2 写入临时文件

将临时文件写入磁盘

~~~java
// 将部分去重结果写入临时文件
    private static File writeTempFile(Set<String> uniqueStrings) throws IOException {
        // UUID防止文件名重复
        File tempFile = new File(TEMP_DIR, UUID.randomUUID().toString() + ".tmp");
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(tempFile))) {
            for (String str : uniqueStrings) {
                writer.write(str);
                writer.newLine();
            }
        }
        return tempFile;
    }
~~~

> 这里需要保证足够的磁盘空间

#### 2.3 合并小文件

在去重合并小文件的也需要考虑到内存大小的问题，可用多路归并排序来避免内存限制

**什么是多路归并排序？**

多路归并排序为将一个大文件分为若干个小文件，将每一个小文件排序，使用**优先级队列**来保存每一个小文件的头部。最终在归并排序的时候一直取队列的头部，这样最终可以得到一个排序好的大文件。多路归并排序一共分为两类排序：一 每个小文件的内部排序，可以使用任意的排序方式如快速排序、堆排序、冒泡排序等；二 小文件合并为大文件排序，需要借助优先队列来维护一个最小序列。

~~~java
// 合并并去重所有临时文件，输出到最终文件
    private static void mergeAndDeduplicateFiles(List<File> tempFiles, String outputFile) throws IOException {
        PriorityQueue<BufferedReader> queue = new PriorityQueue<>(Comparator.comparing(DeduplicateStrings::peekLine));
        Map<BufferedReader, String> currentLines = new HashMap<>();

        // 打开所有临时文件
        for (File tempFile : tempFiles) {
            BufferedReader reader = new BufferedReader(new FileReader(tempFile));
            String line = reader.readLine();
            if (line != null) {
                currentLines.put(reader, line);
                queue.add(reader);
            }
        }

        try (BufferedWriter writer = new BufferedWriter(new FileWriter(outputFile))) {
            String lastLine = null;
            while (!queue.isEmpty()) {
                BufferedReader reader = queue.poll();
                String currentLine = currentLines.get(reader);
                
                // 去重并写入
                if (!currentLine.equals(lastLine)) {
                    writer.write(currentLine);
                    writer.newLine();
                    lastLine = currentLine;
                }

                // 读取下一行
                String nextLine = reader.readLine();
                if (nextLine != null) {
                    currentLines.put(reader, nextLine);
                    queue.add(reader);
                } else {
                    currentLines.remove(reader);
                    reader.close();
                }
            }
        }
    }

// 从BufferedReader中获取当前行
    private static String peekLine(BufferedReader reader) {
        return reader != null ? reader.lines().findFirst().orElse(null) : null;
    }
~~~

### 三.多线程优化

在单核内存限制下可以用这样的处理方式，如果可以多核的话，可以考虑使用多线程来提升处理的速度。也就是使用线程池来处理前面的小文件的写入，让主线程阻塞等待处理小文件合并为最终的一个大文件的部分。注意使用线程安全的List：`Collections.synchronizedList`

```java
package cn.xiaomurui.demo.bigFile.Thread;

import java.io.*;
import java.nio.file.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;

public class DeduplicateStrings {
    // 服务器文件路径模板
    private static final String[] SERVER_FILE_PATHS = {
        "/path/to/server1/file%d.txt",
        "/path/to/server2/file%d.txt"
        // 添加其他服务器路径
    };

    // 临时文件存储目录
    private static final String TEMP_DIR = "temp_files";
    private static final int THREAD_POOL_SIZE = 10;

    public static void main(String[] args) throws IOException, InterruptedException {
        // 创建临时文件目录
        Files.createDirectories(Paths.get(TEMP_DIR));

        List<File> tempFiles = Collections.synchronizedList(new ArrayList<>());
        ExecutorService executor = Executors.newFixedThreadPool(THREAD_POOL_SIZE);

        AtomicInteger fileCounter = new AtomicInteger();

        // 提交每个服务器文件的处理任务到线程池
        for (String serverFilePath : SERVER_FILE_PATHS) {
            for (int i = 1; i <= 10; i++) {
                String filePath = String.format(serverFilePath, i);
                executor.submit(() -> {
                    try {
                        tempFiles.addAll(processFile(filePath, fileCounter.incrementAndGet()));
                    } catch (IOException e) {
                        e.printStackTrace();
                    }
                });
            }
        }

        // 关闭线程池并等待所有任务完成
        executor.shutdown();
        executor.awaitTermination(1, TimeUnit.HOURS);

        // 合并所有临时文件并去重
        mergeAndDeduplicateFiles(tempFiles, "output.txt");

        // 清理临时文件
        for (File tempFile : tempFiles) {
            tempFile.delete();
        }

        // 删除临时文件目录
        Files.delete(Paths.get(TEMP_DIR));
    }

    // 处理单个文件，将去重后的部分结果存储到多个临时文件
    private static List<File> processFile(String filePath, int fileIndex) throws IOException {
        List<File> tempFiles = new ArrayList<>();
        Set<String> uniqueStrings = new HashSet<>();
        int batchSize = 100000; // 每批次处理的行数，可根据内存调整
        int count = 0;

        try (BufferedReader reader = new BufferedReader(new FileReader(filePath))) {
            String line;
            while ((line = reader.readLine()) != null) {
                uniqueStrings.add(line);
                count++;
                if (count >= batchSize) {
                    tempFiles.add(writeTempFile(uniqueStrings, fileIndex));
                    uniqueStrings.clear();
                    count = 0;
                }
            }
            if (!uniqueStrings.isEmpty()) {
                tempFiles.add(writeTempFile(uniqueStrings, fileIndex));
            }
        } catch (FileNotFoundException e) {
            System.out.println("File not found: " + filePath);
        }

        return tempFiles;
    }

    // 将部分去重结果写入临时文件
    private static File writeTempFile(Set<String> uniqueStrings, int fileIndex) throws IOException {
        File tempFile = new File(TEMP_DIR, fileIndex + "_" + UUID.randomUUID().toString() + ".tmp");
        try (BufferedWriter writer = new BufferedWriter(new FileWriter(tempFile))) {
            for (String str : uniqueStrings) {
                writer.write(str);
                writer.newLine();
            }
        }
        return tempFile;
    }

    // 合并并去重所有临时文件，输出到最终文件
    private static void mergeAndDeduplicateFiles(List<File> tempFiles, String outputFile) throws IOException {
        PriorityQueue<BufferedReader> queue = new PriorityQueue<>(Comparator.comparing(DeduplicateStrings::peekLine));
        Map<BufferedReader, String> currentLines = new HashMap<>();

        // 打开所有临时文件
        for (File tempFile : tempFiles) {
            BufferedReader reader = new BufferedReader(new FileReader(tempFile));
            String line = reader.readLine();
            if (line != null) {
                currentLines.put(reader, line);
                queue.add(reader);
            }
        }

        try (BufferedWriter writer = new BufferedWriter(new FileWriter(outputFile))) {
            String lastLine = null;
            while (!queue.isEmpty()) {
                BufferedReader reader = queue.poll();
                String currentLine = currentLines.get(reader);

                // 去重并写入
                if (!currentLine.equals(lastLine)) {
                    writer.write(currentLine);
                    writer.newLine();
                    lastLine = currentLine;
                }

                // 读取下一行
                String nextLine = reader.readLine();
                if (nextLine != null) {
                    currentLines.put(reader, nextLine);
                    queue.add(reader);
                } else {
                    currentLines.remove(reader);
                    reader.close();
                }
            }
        }
    }

    // 从BufferedReader中获取当前行
    private static String peekLine(BufferedReader reader) {
        return reader != null ? reader.lines().findFirst().orElse(null) : null;
    }
}
```

### 四 .总结

1. 大文件的关键在于拆分为内存可以接收的小文件

2. 合并的时候要多利用多路归并的思想避免内存的溢出

3. 可以用多线程优化小文件的排序和分割部分内容，主线程处理归并

4. 最终注意多线程的并发安全用 `AtomicInteger`计数 `Collections.synchronizedList`来做小文件的统计。

   